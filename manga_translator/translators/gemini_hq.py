import os
import re
import asyncio
import base64
import json
from io import BytesIO
from typing import List, Dict, Any
from PIL import Image
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from .common import CommonTranslator, VALID_LANGUAGES, draw_text_boxes_on_image, parse_json_or_text_response, parse_hq_response, get_glossary_extraction_prompt, merge_glossary_to_file, validate_gemini_response
from .keys import GEMINI_API_KEY
from ..utils import Context

# æµè§ˆå™¨é£æ ¼çš„è¯·æ±‚å¤´ï¼Œé¿å…è¢« CF æ‹¦æˆª
BROWSER_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7",
    "Origin": "https://aistudio.google.com",
    "Referer": "https://aistudio.google.com/",
}


def encode_image_for_gemini(image, max_size=1024):
    """å°†å›¾ç‰‡å¤„ç†ä¸ºé€‚åˆGemini APIçš„æ ¼å¼"""
    # è½¬æ¢å›¾ç‰‡æ ¼å¼ä¸ºRGBï¼ˆå¤„ç†æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡æ¨¡å¼ï¼‰
    if image.mode == "P":
        # è°ƒè‰²æ¿æ¨¡å¼ï¼šè½¬æ¢ä¸ºRGBAï¼ˆå¦‚æœæœ‰é€æ˜åº¦ï¼‰æˆ–RGB
        image = image.convert("RGBA" if "transparency" in image.info else "RGB")

    if image.mode == "RGBA":
        # RGBAæ¨¡å¼ï¼šåˆ›å»ºç™½è‰²èƒŒæ™¯å¹¶åˆå¹¶é€æ˜é€šé“
        background = Image.new('RGB', image.size, (255, 255, 255))
        background.paste(image, mask=image.split()[-1])
        image = background
    elif image.mode in ("LA", "L", "1", "CMYK"):
        # LAï¼ˆç°åº¦+é€æ˜ï¼‰ã€Lï¼ˆç°åº¦ï¼‰ã€1ï¼ˆäºŒå€¼ï¼‰ã€CMYKï¼šç»Ÿä¸€è½¬æ¢ä¸ºRGB
        if image.mode == "LA":
            # ç°åº¦+é€æ˜ï¼šå…ˆè½¬RGBAå†åˆå¹¶åˆ°ç™½è‰²èƒŒæ™¯
            image = image.convert("RGBA")
            background = Image.new('RGB', image.size, (255, 255, 255))
            background.paste(image, mask=image.split()[-1])
            image = background
        else:
            # å…¶ä»–æ¨¡å¼ï¼šç›´æ¥è½¬RGB
            image = image.convert("RGB")
    elif image.mode != "RGB":
        # å…¶ä»–æœªçŸ¥æ¨¡å¼ï¼šå¼ºåˆ¶è½¬æ¢ä¸ºRGB
        image = image.convert("RGB")

    # è°ƒæ•´å›¾ç‰‡å¤§å°
    w, h = image.size
    if max(w, h) > max_size:
        scale = max_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        image = image.resize((new_w, new_h), Image.LANCZOS)

    return image


def _flatten_prompt_data(data: Any, indent: int = 0) -> str:
    """Recursively flattens a dictionary or list into a formatted string."""
    prompt_parts = []
    prefix = "  " * indent

    if isinstance(data, dict):
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                prompt_parts.append(f"{prefix}- {key}:")
                prompt_parts.append(_flatten_prompt_data(value, indent + 1))
            else:
                prompt_parts.append(f"{prefix}- {key}: {value}")
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, (dict, list)):
                prompt_parts.append(_flatten_prompt_data(item, indent + 1))
            else:
                prompt_parts.append(f"{prefix}- {item}")
    
    return "\n".join(prompt_parts)

class GeminiHighQualityTranslator(CommonTranslator):
    """
    Geminié«˜è´¨é‡ç¿»è¯‘å™¨
    æ”¯æŒå¤šå›¾ç‰‡æ‰¹é‡å¤„ç†ï¼Œæä¾›æ–‡æœ¬æ¡†é¡ºåºã€åŸæ–‡å’ŒåŸå›¾ç»™AIè¿›è¡Œæ›´ç²¾å‡†çš„ç¿»è¯‘
    """
    _LANGUAGE_CODE_MAP = VALID_LANGUAGES
    
    # ç±»å˜é‡: è·¨å®ä¾‹å…±äº«çš„RPMé™åˆ¶æ—¶é—´æˆ³
    _GLOBAL_LAST_REQUEST_TS = {}  # {model_name: timestamp}
    
    def __init__(self):
        super().__init__()
        self.client = None
        self.prev_context = ""  # ç”¨äºå­˜å‚¨å¤šé¡µä¸Šä¸‹æ–‡
        # Initial setup from environment variables
        # åªåœ¨éWebç¯å¢ƒä¸‹é‡æ–°åŠ è½½.envæ–‡ä»¶
        is_web_server = os.getenv('MANGA_TRANSLATOR_WEB_SERVER', 'false').lower() == 'true'
        if not is_web_server:
            from dotenv import load_dotenv
            load_dotenv(override=True)
        
        self.api_key = os.getenv('GEMINI_API_KEY', GEMINI_API_KEY)
        self.base_url = os.getenv('GEMINI_API_BASE', 'https://generativelanguage.googleapis.com')
        self.model_name = os.getenv('GEMINI_MODEL', "gemini-1.5-flash")
        self.max_tokens = None  # ä¸é™åˆ¶ï¼Œä½¿ç”¨æ¨¡å‹é»˜è®¤æœ€å¤§å€¼
        self.temperature = 0.1
        self._MAX_REQUESTS_PER_MINUTE = 0  # é»˜è®¤æ— é™åˆ¶
        # ä½¿ç”¨å…¨å±€æ—¶é—´æˆ³,è·¨å®ä¾‹å…±äº«
        if self.model_name not in GeminiHighQualityTranslator._GLOBAL_LAST_REQUEST_TS:
            GeminiHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self.model_name] = 0
        self._last_request_ts_key = self.model_name
        self.safety_settings = [
            {
                "category": HarmCategory.HARM_CATEGORY_HARASSMENT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
        ]
        self._setup_client()
    
    def set_prev_context(self, context: str):
        """è®¾ç½®å¤šé¡µä¸Šä¸‹æ–‡ï¼ˆç”¨äºcontext_size > 0æ—¶ï¼‰"""
        self.prev_context = context if context else ""
    
    def parse_args(self, args):
        """è§£æé…ç½®å‚æ•°"""
        # è°ƒç”¨çˆ¶ç±»çš„ parse_args æ¥è®¾ç½®é€šç”¨å‚æ•°ï¼ˆåŒ…æ‹¬ attemptsã€post_check ç­‰ï¼‰
        super().parse_args(args)
        
        # åŒæ­¥ attempts åˆ° _max_total_attempts
        self._max_total_attempts = self.attempts
        
        # ä»é…ç½®ä¸­è¯»å–RPMé™åˆ¶
        max_rpm = getattr(args, 'max_requests_per_minute', 0)
        if max_rpm > 0:
            self._MAX_REQUESTS_PER_MINUTE = max_rpm
            self.logger.info(f"Setting Gemini HQ max requests per minute to: {max_rpm}")
        
        # ä»é…ç½®ä¸­è¯»å–ç”¨æˆ·çº§ API Keyï¼ˆä¼˜å…ˆäºç¯å¢ƒå˜é‡ï¼‰
        # è¿™å…è®¸ Web æœåŠ¡å™¨ä¸ºæ¯ä¸ªç”¨æˆ·ä½¿ç”¨ä¸åŒçš„ API Key
        need_rebuild_client = False
        
        user_api_key = getattr(args, 'user_api_key', None)
        if user_api_key and user_api_key != self.api_key:
            self.api_key = user_api_key
            need_rebuild_client = True
            self.logger.info("[UserAPIKey] Using user-provided API key for Gemini HQ")
        
        user_api_base = getattr(args, 'user_api_base', None)
        if user_api_base and user_api_base != self.base_url:
            self.base_url = user_api_base
            need_rebuild_client = True
            self.logger.info(f"[UserAPIKey] Using user-provided API base: {user_api_base}")
        
        user_api_model = getattr(args, 'user_api_model', None)
        if user_api_model:
            self.model_name = user_api_model
            # æ›´æ–°å…¨å±€æ—¶é—´æˆ³çš„ key
            if self.model_name not in GeminiHighQualityTranslator._GLOBAL_LAST_REQUEST_TS:
                GeminiHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self.model_name] = 0
            self._last_request_ts_key = self.model_name
            self.logger.info(f"[UserAPIKey] Using user-provided model: {user_api_model}")
        
        # å¦‚æœ API Key æˆ– Base URL å˜åŒ–ï¼Œé‡å»ºå®¢æˆ·ç«¯
        if need_rebuild_client:
            self.client = None
            self._setup_client()
    
    def _setup_client(self, system_instruction=None):
        """è®¾ç½®Geminiå®¢æˆ·ç«¯"""
        if not self.client and self.api_key:
            # æ„å»º client_optionsï¼Œæ·»åŠ æµè§ˆå™¨é£æ ¼è¯·æ±‚å¤´é¿å… CF æ‹¦æˆª
            client_options = {}
            if self.base_url:
                client_options["api_endpoint"] = self.base_url
            
            # é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼ˆgoogle-api-core æ”¯æŒï¼‰
            import os
            os.environ.setdefault('GOOGLE_API_USE_CLIENT_CERTIFICATE', 'false')

            genai.configure(
                api_key=self.api_key,
                transport='rest',  # æ”¯æŒè‡ªå®šä¹‰base_url
                client_options=client_options if client_options else None,
                default_metadata=[
                    ("user-agent", BROWSER_HEADERS["User-Agent"]),
                    ("accept", BROWSER_HEADERS["Accept"]),
                    ("accept-language", BROWSER_HEADERS["Accept-Language"]),
                ]
            )
            
            # ç»Ÿä¸€é…ç½®ï¼ˆä¸åœ¨å®¢æˆ·ç«¯åˆå§‹åŒ–æ—¶åŒ…å«å®‰å…¨è®¾ç½®ï¼‰
            # å®‰å…¨è®¾ç½®å°†åœ¨æ¯æ¬¡è¯·æ±‚æ—¶åŠ¨æ€æ·»åŠ ï¼Œå¦‚æœæŠ¥é”™åˆ™è‡ªåŠ¨å›é€€
            generation_config = {
                "temperature": self.temperature,
                "top_p": 0.95,
                "top_k": 64,
                "max_output_tokens": self.max_tokens,
                "response_mime_type": "text/plain",
            }
            model_args = {
                "model_name": self.model_name,
                "generation_config": generation_config,
            }
            
            # å¦‚æœæä¾›äº†ç³»ç»ŸæŒ‡ä»¤ï¼Œåˆ™æ·»åŠ åˆ°æ¨¡å‹é…ç½®ä¸­
            if system_instruction:
                model_args["system_instruction"] = system_instruction
                self.logger.info(f"Gemini HQå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨ system_instructionï¼‰ã€‚Base URL: {self.base_url or 'é»˜è®¤'}")
            else:
                self.logger.info(f"Gemini HQå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆã€‚Base URL: {self.base_url or 'é»˜è®¤'}")
            
            self.logger.info(f"å®‰å…¨è®¾ç½®ç­–ç•¥ï¼šé»˜è®¤å‘é€ BLOCK_NONEï¼Œå¦‚é‡é”™è¯¯è‡ªåŠ¨å›é€€")

            self.client = genai.GenerativeModel(**model_args)
    

    
    def _build_system_prompt(self, source_lang: str, target_lang: str, custom_prompt_json: Dict[str, Any] = None, line_break_prompt_json: Dict[str, Any] = None, retry_attempt: int = 0, retry_reason: str = "", extract_glossary: bool = False) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        # Map language codes to full names for clarity in the prompt
        lang_map = {
            "CHS": "Simplified Chinese",
            "CHT": "Traditional Chinese",
            "JPN": "Japanese",
            "ENG": "English",
            "KOR": "Korean",
            "VIN": "Vietnamese",
            "FRA": "French",
            "DEU": "German",
            "ITA": "Italian",
        }
        target_lang_full = lang_map.get(target_lang, target_lang) # Fallback to the code itself

        custom_prompt_str = ""
        if custom_prompt_json:
            custom_prompt_str = _flatten_prompt_data(custom_prompt_json)
            # self.logger.info(f"--- Custom Prompt Content ---\n{custom_prompt_str}\n---------------------------")

        line_break_prompt_str = ""
        if line_break_prompt_json and line_break_prompt_json.get('line_break_prompt'):
            line_break_prompt_str = line_break_prompt_json['line_break_prompt']

        try:
            from ..utils import BASE_PATH
            import os
            import json
            prompt_path = os.path.join(BASE_PATH, 'dict', 'system_prompt_hq.json')
            with open(prompt_path, 'r', encoding='utf-8') as f:
                base_prompt_data = json.load(f)
            base_prompt = base_prompt_data['system_prompt']
        except Exception as e:
            self.logger.warning(f"Failed to load system prompt from file, falling back to hardcoded prompt. Error: {e}")
            base_prompt = f"""You are an expert manga translator. Your task is to accurately translate manga text from the source language into **{{{target_lang}}}**. You will be given the full manga page for context.\n\n**CRITICAL INSTRUCTIONS (FOLLOW STRICTLY):**\n\n1.  **DIRECT TRANSLATION ONLY**: Your output MUST contain ONLY the raw, translated text. Nothing else.\n    -   DO NOT include the original text.\n    -   DO NOT include any explanations, greetings, apologies, or any conversational text.\n    -   DO NOT use Markdown formatting (like ```json or ```).\n    -   The output is fed directly to an automated script. Any extra text will cause it to fail.\n
2.  **MATCH LINE COUNT**: The number of lines in your output MUST EXACTLY match the number of text regions you are asked to translate. Each line in your output corresponds to one numbered text region in the input.\n
3.  **TRANSLATE EVERYTHING**: Translate all text provided, including sound effects and single characters. Do not leave any line untranslated.\n
4.  **ACCURACY AND TONE**:\n    -   Preserve the original tone, emotion, and character's voice.\n    -   Ensure consistent translation of names, places, and special terms.\n    -   For onomatopoeia (sound effects), provide the equivalent sound in {{{target_lang}}} or a brief description (e.g., '(rumble)', '(thud)').\n\n---\n\n**EXAMPLE OF CORRECT AND INCORRECT OUTPUT:**\n\n**[ CORRECT OUTPUT EXAMPLE ]**\nThis is a correct response. Notice it only contains the translated text, with each translation on a new line.\n\n(Imagine the user input was: "1. ã†ã‚‹ã•ã„ï¼", "2. é»™ã‚Œï¼")\n```\nåµæ­»äº†ï¼\né—­å˜´ï¼\n```\n\n**[ âŒ INCORRECT OUTPUT EXAMPLE ]**\nThis is an incorrect response because it includes extra text and explanations.\n\n(Imagine the user input was: "1. ã†ã‚‹ã•ã„ï¼", "2. é»™ã‚Œï¼")\n```\nå¥½çš„ï¼Œè¿™æ˜¯æ‚¨çš„ç¿»è¯‘ï¼š\n1. åµæ­»äº†ï¼
2. é—­å˜´ï¼
```\n**REASONING:** The above example is WRONG because it includes "å¥½çš„ï¼Œè¿™æ˜¯æ‚¨çš„ç¿»è¯‘ï¼š" and numbering. Your response must be ONLY the translated text, line by line.\n\n---\n\n**FINAL INSTRUCTION:** Now, perform the translation task. Remember, your response must be clean, containing only the translated text."""

        # Replace placeholder with the full language name
        base_prompt = base_prompt.replace("{{{target_lang}}}", target_lang_full)
        
        # Also replace target_lang placeholder in custom prompt
        if custom_prompt_str:
            custom_prompt_str = custom_prompt_str.replace("{{{target_lang}}}", target_lang_full)

        # Combine prompts
        final_prompt = ""
        
        # æ·»åŠ é‡è¯•æç¤ºåˆ°æœ€å‰é¢ï¼ˆå¦‚æœæ˜¯é‡è¯•ï¼‰
        if retry_attempt > 0:
            final_prompt += self._get_retry_hint(retry_attempt, retry_reason) + "\n"
        
        if line_break_prompt_str:
            final_prompt += f"{line_break_prompt_str}\n\n---\n\n"
        if custom_prompt_str:
            final_prompt += f"{custom_prompt_str}\n\n---\n\n"
        
        final_prompt += base_prompt
        
        # è¿½åŠ æœ¯è¯­æå–æç¤ºè¯
        if extract_glossary:
            extraction_prompt = get_glossary_extraction_prompt(target_lang_full)
            if extraction_prompt:
                final_prompt += f"\n\n---\n\n{extraction_prompt}"
                self.logger.info("å·²å¯ç”¨è‡ªåŠ¨æœ¯è¯­æå–ï¼Œæç¤ºè¯å·²è¿½åŠ ã€‚")
        
        return final_prompt

    def _build_user_prompt(self, batch_data: List[Dict], ctx: Any, retry_attempt: int = 0, retry_reason: str = "") -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆé«˜è´¨é‡ç‰ˆï¼‰- ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•ï¼ŒåªåŒ…å«ä¸Šä¸‹æ–‡å’Œå¾…ç¿»è¯‘æ–‡æœ¬"""
        return self._build_user_prompt_for_hq(batch_data, ctx, self.prev_context, retry_attempt=retry_attempt, retry_reason=retry_reason)
    
    def _get_system_instruction(self, source_lang: str, target_lang: str, custom_prompt_json: Dict[str, Any] = None, line_break_prompt_json: Dict[str, Any] = None, retry_attempt: int = 0, retry_reason: str = "", extract_glossary: bool = False) -> str:
        """è·å–å®Œæ•´çš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆåŒ…å«æ–­å¥æç¤ºè¯ã€è‡ªå®šä¹‰æç¤ºè¯å’ŒåŸºç¡€ç³»ç»Ÿæç¤ºè¯ï¼‰"""
        # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«æ‰€æœ‰æŒ‡ä»¤ï¼‰
        return self._build_system_prompt(source_lang, target_lang, custom_prompt_json=custom_prompt_json, line_break_prompt_json=line_break_prompt_json, retry_attempt=retry_attempt, retry_reason=retry_reason, extract_glossary=extract_glossary)

    async def _translate_batch_high_quality(self, texts: List[str], batch_data: List[Dict], source_lang: str, target_lang: str, custom_prompt_json: Dict[str, Any] = None, line_break_prompt_json: Dict[str, Any] = None, ctx: Any = None, split_level: int = 0) -> List[str]:
        """é«˜è´¨é‡æ‰¹é‡ç¿»è¯‘æ–¹æ³•"""
        if not texts:
            return []
        
        # ä¿å­˜å‚æ•°ä¾›é‡è¯•æ—¶ä½¿ç”¨
        _source_lang = source_lang
        _target_lang = target_lang
        _custom_prompt_json = custom_prompt_json
        _line_break_prompt_json = line_break_prompt_json
        
        # æ‰“å°è¾“å…¥çš„åŸæ–‡
        self.logger.info("--- Original Texts for Translation ---")
        for i, text in enumerate(texts):
            self.logger.info(f"{i+1}: {text}")
        self.logger.info("------------------------------------")

        # æ‰“å°å›¾ç‰‡ä¿¡æ¯
        self.logger.info("--- Image Info ---")
        for i, data in enumerate(batch_data):
            image = data['image']
            self.logger.info(f"Image {i+1}: size={image.size}, mode={image.mode}")
        self.logger.info("--------------------")

        # å‡†å¤‡å›¾ç‰‡åˆ—è¡¨ï¼ˆæ”¾åœ¨æœ€åï¼‰
        image_parts = []
        for data in batch_data:
            image = data['image']
            
            # åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶å¸¦ç¼–å·çš„æ–‡æœ¬æ¡†
            text_regions = data.get('text_regions', [])
            text_order = data.get('text_order', [])
            upscaled_size = data.get('upscaled_size')
            if text_regions and text_order:
                # å°†PILå›¾ç‰‡è½¬æ¢ä¸ºnumpyæ•°ç»„
                import numpy as np
                image_array = np.array(image)
                # ç»˜åˆ¶æ–‡æœ¬æ¡†ï¼ˆä¼ å…¥è¶…åˆ†å°ºå¯¸ç”¨äºåæ ‡è½¬æ¢ï¼‰
                image_array = draw_text_boxes_on_image(image_array, text_regions, text_order, upscaled_size)
                # è½¬æ¢å›PILå›¾ç‰‡
                from PIL import Image as PILImage
                image = PILImage.fromarray(image_array)
                self.logger.debug(f"å·²åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶ {len(text_regions)} ä¸ªå¸¦ç¼–å·çš„æ–‡æœ¬æ¡†")
            
            processed_image = encode_image_for_gemini(image)
            image_parts.append(processed_image)
        
        # åˆå§‹åŒ–é‡è¯•ä¿¡æ¯
        retry_attempt = 0
        retry_reason = ""
        
        # å‘é€è¯·æ±‚
        max_retries = self.attempts
        attempt = 0
        is_infinite = max_retries == -1
        last_exception = None
        local_attempt = 0  # æœ¬æ¬¡æ‰¹æ¬¡çš„å°è¯•æ¬¡æ•°
        
        # æ ‡è®°æ˜¯å¦éœ€è¦å›é€€ï¼ˆä¸å‘é€å®‰å…¨è®¾ç½®ï¼‰
        should_retry_without_safety = False
        
        # æ ‡è®°æ˜¯å¦å‘é€å›¾ç‰‡ï¼ˆé™çº§æœºåˆ¶ï¼‰
        send_images = True

        def generate_content_with_logging(**kwargs):
            # æ‰“å°è¯·æ±‚ä½“ï¼ˆå»é™¤å›¾ç‰‡æ•°æ®ï¼‰- å·²æ³¨é‡Šä»¥å‡å°‘æ—¥å¿—è¾“å‡º
            # log_kwargs = kwargs.copy()
            # if 'contents' in log_kwargs and isinstance(log_kwargs['contents'], list):
            #     serializable_contents = []
            #     for item in log_kwargs['contents']:
            #         if isinstance(item, Image.Image):
            #             serializable_contents.append(f"<PIL.Image.Image size={item.size} mode={item.mode}>")
            #         else:
            #             serializable_contents.append(item)
            #     log_kwargs['contents'] = serializable_contents
            # 
            # self.logger.info(f"--- Gemini Request Body ---\n{json.dumps(log_kwargs, indent=2, ensure_ascii=False)}\n---------------------------")
            return self.client.generate_content(**kwargs)

        while is_infinite or attempt < max_retries:
            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            self._check_cancelled()
            
            # æ£€æŸ¥å…¨å±€å°è¯•æ¬¡æ•°
            if not self._increment_global_attempt():
                self.logger.error("Reached global attempt limit. Stopping translation.")
                # åŒ…å«æœ€åä¸€æ¬¡é”™è¯¯çš„çœŸæ­£åŸå› 
                last_error_msg = str(last_exception) if last_exception else "Unknown error"
                raise Exception(f"è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({self._max_total_attempts})ï¼Œæœ€åä¸€æ¬¡é”™è¯¯: {last_error_msg}")

            local_attempt += 1
            attempt += 1

            # æ–‡æœ¬åˆ†å‰²é€»è¾‘å·²ç¦ç”¨
            # if local_attempt > self._SPLIT_THRESHOLD and len(texts) > 1 and split_level < self._MAX_SPLIT_ATTEMPTS:
            #     self.logger.warning(f"Triggering split after {local_attempt} local attempts")
            #     raise self.SplitException(local_attempt, texts)
            
            # ç¡®å®šæ˜¯å¦å¼€å¯æœ¯è¯­æå–
            # å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š1. æœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼ˆæ‰æœ‰åœ°æ–¹å­˜ï¼‰ 2. é…ç½®å¼€å¯äº†æå–å¼€å…³
            config_extract = False
            if ctx and hasattr(ctx, 'config') and hasattr(ctx.config, 'translator'):
                config_extract = getattr(ctx.config.translator, 'extract_glossary', False)
            
            extract_glossary = bool(_custom_prompt_json) and config_extract

            # è·å–ç³»ç»ŸæŒ‡ä»¤ï¼ˆä¸å†ç”¨äºåˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œè€Œæ˜¯åˆå¹¶åˆ°ç”¨æˆ·æ¶ˆæ¯ï¼‰
            system_instruction = self._get_system_instruction(_source_lang, _target_lang, custom_prompt_json=_custom_prompt_json, line_break_prompt_json=_line_break_prompt_json, retry_attempt=retry_attempt, retry_reason=retry_reason, extract_glossary=extract_glossary)
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆä¸ä¼ å…¥ system_instructionï¼‰
            self.client = None
            self._setup_client(system_instruction=None)
            
            if not self.client:
                self.logger.error("Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
                return texts
            
            # æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆåŒ…å«é‡è¯•ä¿¡æ¯ä»¥é¿å…ç¼“å­˜ï¼‰
            user_prompt = self._build_user_prompt(batch_data, ctx, retry_attempt=retry_attempt, retry_reason=retry_reason)
            
            # å°†ç³»ç»Ÿæç¤ºè¯åˆå¹¶åˆ°ç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
            combined_prompt = f"{system_instruction}\n\n{user_prompt}"
            
            # å‡†å¤‡å†…å®¹ï¼šuseræ¶ˆæ¯åŒ…å«ç³»ç»Ÿæç¤ºè¯ã€ä¸Šä¸‹æ–‡ã€å¾…ç¿»è¯‘æ–‡æœ¬å’Œå›¾ç‰‡
            # é™çº§æ£€æŸ¥ï¼šå¦‚æœ send_images ä¸º Falseï¼Œåˆ™ä¸å‘é€å›¾ç‰‡
            if send_images:
                content_parts = [combined_prompt] + image_parts
            else:
                if retry_attempt > 0: # ä»…åœ¨é‡è¯•ä¸”è¢«æ ‡è®°ä¸ºä¸å‘å›¾æ—¶æ‰“å°
                     self.logger.warning("é™çº§æ¨¡å¼ï¼šä»…å‘é€æ–‡æœ¬ï¼Œä¸å‘é€å›¾ç‰‡")
                content_parts = [combined_prompt]
            
            # åŠ¨æ€æ„å»ºè¯·æ±‚å‚æ•° - é»˜è®¤æ€»æ˜¯å‘é€å®‰å…¨è®¾ç½®
            request_args = {
                "contents": content_parts,
                "safety_settings": self.safety_settings
            }

            try:
                # RPMé™åˆ¶
                if self._MAX_REQUESTS_PER_MINUTE > 0:
                    import time
                    now = time.time()
                    delay = 60.0 / self._MAX_REQUESTS_PER_MINUTE
                    elapsed = now - GeminiHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self._last_request_ts_key]
                    if elapsed < delay:
                        sleep_time = delay - elapsed
                        self.logger.info(f'Ratelimit sleep: {sleep_time:.2f}s')
                        await asyncio.sleep(sleep_time)
                
                # å¦‚æœéœ€è¦å›é€€ï¼Œç§»é™¤å®‰å…¨è®¾ç½®
                if should_retry_without_safety and "safety_settings" in request_args:
                    self.logger.warning("å›é€€æ¨¡å¼ï¼šç§»é™¤å®‰å…¨è®¾ç½®å‚æ•°")
                    request_args = {k: v for k, v in request_args.items() if k != "safety_settings"}
                
                # åŠ¨æ€è°ƒæ•´æ¸©åº¦ï¼šè´¨é‡æ£€æŸ¥æˆ–BRæ£€æŸ¥å¤±è´¥æ—¶æé«˜æ¸©åº¦å¸®åŠ©è·³å‡ºé”™è¯¯æ¨¡å¼
                current_temperature = self._get_retry_temperature(self.temperature, retry_attempt, retry_reason)
                if retry_attempt > 0 and current_temperature != self.temperature:
                    self.logger.info(f"[é‡è¯•] æ¸©åº¦è°ƒæ•´: {self.temperature} -> {current_temperature}")
                    # è¦†ç›– generation_config ä¸­çš„æ¸©åº¦
                    request_args["generation_config"] = {"temperature": current_temperature}
                
                # è®¾ç½®5åˆ†é’Ÿè¶…æ—¶
                request_args["request_options"] = {"timeout": 300}
                response = await asyncio.to_thread(
                    generate_content_with_logging,
                    **request_args
                )
                
                # åœ¨APIè°ƒç”¨æˆåŠŸåç«‹å³æ›´æ–°æ—¶é—´æˆ³ï¼Œç¡®ä¿æ‰€æœ‰è¯·æ±‚ï¼ˆåŒ…æ‹¬é‡è¯•ï¼‰éƒ½è¢«è®¡å…¥é€Ÿç‡é™åˆ¶
                if self._MAX_REQUESTS_PER_MINUTE > 0:
                    GeminiHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self._last_request_ts_key] = time.time()

                # éªŒè¯å“åº”å¯¹è±¡æ˜¯å¦æœ‰æ•ˆ
                validate_gemini_response(response, self.logger)

                # æ£€æŸ¥finish_reasonï¼Œåªæœ‰æˆåŠŸ(1)æ‰ç»§ç»­ï¼Œå…¶ä»–éƒ½é‡è¯•
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = candidate.finish_reason

                        if finish_reason != 1:  # ä¸æ˜¯STOP(æˆåŠŸ)
                            attempt += 1
                            log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"

                            # æ˜¾ç¤ºå…·ä½“çš„finish_reasonä¿¡æ¯
                            finish_reason_map = {
                                1: "STOP(æˆåŠŸ)",
                                2: "SAFETY(å®‰å…¨ç­–ç•¥æ‹¦æˆª)",
                                3: "MAX_TOKENS(è¾¾åˆ°æœ€å¤§tokené™åˆ¶)",
                                4: "RECITATION(å†…å®¹é‡å¤æ£€æµ‹)",
                                5: "OTHER(å…¶ä»–æœªçŸ¥é”™è¯¯)"
                            }
                            reason_desc = finish_reason_map.get(finish_reason, f"æœªçŸ¥é”™è¯¯ç ({finish_reason})")

                            self.logger.warning(f"Gemini APIå¤±è´¥ ({log_attempt}): finish_reason={finish_reason} - {reason_desc}")
                            
                            # é™çº§ç­–ç•¥ï¼šå¦‚æœæ˜¯å®‰å…¨ç­–ç•¥æ‹¦æˆªæˆ–å…¶ä»–éæˆåŠŸçŠ¶æ€ï¼Œå°è¯•ä¸å‘é€å›¾ç‰‡
                            if finish_reason == 2 or finish_reason == 5: # SAFETY or OTHER
                                self.logger.warning("æ£€æµ‹åˆ°å®‰å…¨ç­–ç•¥æ‹¦æˆªæˆ–æœªçŸ¥é”™è¯¯ï¼Œä¸‹æ¬¡é‡è¯•å°†ä¸å†å‘é€å›¾ç‰‡")
                                send_images = False

                            if not is_infinite and attempt >= max_retries:
                                self.logger.error(f"Geminiç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»å¤±è´¥: {reason_desc}")
                                break
                            await asyncio.sleep(1)
                            continue

                # å°è¯•è®¿é—® .text å±æ€§ï¼Œå¦‚æœAPIå› å®‰å…¨åŸå› ç­‰è¿”å›ç©ºå†…å®¹ï¼Œè¿™é‡Œä¼šè§¦å‘å¼‚å¸¸
                result_text = response.text.strip()
                
                # ç»Ÿä¸€çš„ç¼–ç æ¸…ç†ï¼ˆå¤„ç†UTF-16-LEç­‰ç¼–ç é—®é¢˜ï¼‰
                from .common import sanitize_text_encoding
                result_text = sanitize_text_encoding(result_text)
                
                self.logger.debug(f"--- Gemini Raw Response ---\n{result_text}\n---------------------------")
                if not result_text:
                     # ç©ºå›å¤„ç†ï¼šä¹Ÿå°è¯•é™çº§
                    self.logger.warning("Gemini APIè¿”å›ç©ºæ–‡æœ¬ï¼Œä¸‹æ¬¡é‡è¯•å°†ä¸å†å‘é€å›¾ç‰‡")
                    send_images = False
                    raise Exception("Gemini API returned empty text")


                # ä½¿ç”¨é€šç”¨å‡½æ•°è§£æå“åº”ï¼ˆæ”¯æŒJSONå’Œçº¯æ–‡æœ¬ï¼Œä»¥åŠæœ¯è¯­æå–ï¼‰
                translations, new_terms = parse_hq_response(result_text)
                
                # å¤„ç†æå–åˆ°çš„æœ¯è¯­
                if extract_glossary and new_terms:
                    prompt_path = None
                    if ctx and hasattr(ctx, 'config') and hasattr(ctx.config, 'translator'):
                        prompt_path = getattr(ctx.config.translator, 'high_quality_prompt_path', None)
                    
                    if prompt_path:
                        merge_glossary_to_file(prompt_path, new_terms)
                    else:
                        self.logger.warning("Extracted new terms but prompt path not found in context.")
                
                # Strict validation: must match input count
                if len(translations) != len(texts):
                    retry_attempt += 1
                    retry_reason = f"Translation count mismatch: expected {len(texts)}, got {len(translations)}"
                    log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                    self.logger.warning(f"[{log_attempt}] {retry_reason}. Retrying...")
                    self.logger.warning(f"Expected texts: {texts}")
                    self.logger.warning(f"Got translations: {translations}")
                    
                    # è®°å½•é”™è¯¯ä»¥ä¾¿åœ¨è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°æ—¶æ˜¾ç¤º
                    last_exception = Exception(f"ç¿»è¯‘æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(texts)} æ¡ï¼Œå®é™…å¾—åˆ° {len(translations)} æ¡")

                    if not is_infinite and attempt >= max_retries:
                        raise Exception(f"Translation count mismatch after {max_retries} attempts: expected {len(texts)}, got {len(translations)}")

                    await asyncio.sleep(2)
                    continue

                # è´¨é‡éªŒè¯ï¼šæ£€æŸ¥ç©ºç¿»è¯‘ã€åˆå¹¶ç¿»è¯‘ã€å¯ç–‘ç¬¦å·ç­‰
                is_valid, error_msg = self._validate_translation_quality(texts, translations)
                if not is_valid:
                    retry_attempt += 1
                    retry_reason = f"Quality check failed: {error_msg}"
                    log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                    self.logger.warning(f"[{log_attempt}] {retry_reason}. Retrying...")
                    
                    # è®°å½•é”™è¯¯ä»¥ä¾¿åœ¨è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°æ—¶æ˜¾ç¤º
                    last_exception = Exception(f"ç¿»è¯‘è´¨é‡æ£€æŸ¥å¤±è´¥: {error_msg}")

                    if not is_infinite and attempt >= max_retries:
                        raise Exception(f"Quality check failed after {max_retries} attempts: {error_msg}")

                    await asyncio.sleep(2)
                    continue

                # æ‰“å°åŸæ–‡å’Œè¯‘æ–‡çš„å¯¹åº”å…³ç³»
                self.logger.info("--- Translation Results ---")
                for original, translated in zip(texts, translations):
                    self.logger.info(f'{original} -> {translated}')
                self.logger.info("---------------------------")

                # BRæ£€æŸ¥ï¼šæ£€æŸ¥ç¿»è¯‘ç»“æœæ˜¯å¦åŒ…å«å¿…è¦çš„[BR]æ ‡è®°
                # BR check: Check if translations contain necessary [BR] markers
                if not self._validate_br_markers(translations, batch_data=batch_data, ctx=ctx):
                    retry_attempt += 1
                    retry_reason = "BR markers missing in translations"
                    log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                    self.logger.warning(f"[{log_attempt}] {retry_reason}, retrying...")
                    
                    # è®°å½•é”™è¯¯ä»¥ä¾¿åœ¨è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°æ—¶æ˜¾ç¤º
                    last_exception = Exception("AIæ–­å¥æ£€æŸ¥å¤±è´¥: ç¿»è¯‘ç»“æœç¼ºå°‘å¿…è¦çš„[BR]æ ‡è®°")
                    
                    # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæŠ›å‡ºå‹å¥½çš„å¼‚å¸¸
                    if not is_infinite and attempt >= max_retries:
                        from .common import BRMarkersValidationException
                        self.logger.error("Geminié«˜è´¨é‡ç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥ï¼šAIæ–­å¥æ£€æŸ¥å¤±è´¥ã€‚")
                        raise BRMarkersValidationException(
                            missing_count=0,  # å…·ä½“æ•°å­—åœ¨_validate_br_markersä¸­å·²è®°å½•
                            total_count=len(texts),
                            tolerance=max(1, len(texts) // 10)
                        )
                    
                    await asyncio.sleep(2)
                    continue

                return translations[:len(texts)]

            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯400é”™è¯¯æˆ–å¤šæ¨¡æ€ä¸æ”¯æŒé—®é¢˜
                error_message = str(e)
                last_exception = e  # ä¿å­˜æœ€åä¸€æ¬¡é”™è¯¯
                is_bad_request = '400' in error_message or 'BadRequest' in error_message
                is_multimodal_unsupported = any(keyword in error_message.lower() for keyword in [
                    'image_url', 'multimodal', 'vision', 'expected `text`', 'unknown variant', 'does not support'
                ])
                
                # é™çº§æ£€æŸ¥ï¼š502é”™è¯¯ã€å®‰å…¨è®¾ç½®é”™è¯¯æˆ–400é”™è¯¯ï¼ˆéå¤šæ¨¡æ€ä¸æ”¯æŒï¼‰
                is_502_error = '502' in error_message
                is_safety_error = any(keyword in error_message.lower() for keyword in [
                    'safety_settings', 'safetysettings', 'harm', 'block', 'safety'
                ]) or ("400" in error_message and not is_multimodal_unsupported)

                if is_502_error or is_safety_error:
                     self.logger.warning(f"æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯(502)æˆ–å®‰å…¨è®¾ç½®é”™è¯¯ï¼Œä¸‹æ¬¡é‡è¯•å°†ä¸å†å‘é€å›¾ç‰‡ã€‚é”™è¯¯ä¿¡æ¯: {error_message}")
                     send_images = False

                if is_bad_request and is_multimodal_unsupported:
                    self.logger.error(f"âŒ æ¨¡å‹ {self.model_name} ä¸æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾ç‰‡+æ–‡æœ¬ï¼‰")
                    self.logger.error(f"ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
                    self.logger.error(f"   1. ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„Geminiæ¨¡å‹ï¼ˆå¦‚ gemini-1.5-flash, gemini-1.5-proï¼‰")
                    self.logger.error(f"   2. æˆ–è€…åˆ‡æ¢åˆ°æ™®é€šç¿»è¯‘æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ _hq é«˜è´¨é‡ç¿»è¯‘å™¨ï¼‰")
                    self.logger.error(f"   3. æ£€æŸ¥ç¬¬ä¸‰æ–¹APIæ˜¯å¦æ”¯æŒå›¾ç‰‡è¾“å…¥")
                    raise Exception(f"æ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€è¾“å…¥: {self.model_name}") from e
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®‰å…¨è®¾ç½®ç›¸å…³çš„é”™è¯¯
                # is_safety_error å·²ç»åœ¨ä¸Šé¢å®šä¹‰äº†
                
                # å¦‚æœæ˜¯å®‰å…¨è®¾ç½®é”™è¯¯ä¸”è¿˜æ²¡æœ‰å°è¯•å›é€€ï¼Œåˆ™æ ‡è®°å›é€€
                if is_safety_error and not should_retry_without_safety:
                    self.logger.warning(f"æ£€æµ‹åˆ°å®‰å…¨è®¾ç½®ç›¸å…³é”™è¯¯ï¼Œå°†åœ¨ä¸‹æ¬¡é‡è¯•æ—¶ç§»é™¤å®‰å…¨è®¾ç½®å‚æ•°: {error_message}")
                    should_retry_without_safety = True
                    # ä¸å¢åŠ attemptè®¡æ•°ï¼Œç›´æ¥é‡è¯•
                    await asyncio.sleep(1)
                    continue
                    
                attempt += 1
                log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                self.logger.warning(f"Geminié«˜è´¨é‡ç¿»è¯‘å‡ºé”™ ({log_attempt}): {e}")

                if "finish_reason: 2" in error_message or "finish_reason is 2" in error_message:
                    self.logger.warning("æ£€æµ‹åˆ°Geminiå®‰å…¨ç­–ç•¥æ‹¦æˆªã€‚æ­£åœ¨é‡è¯•...")
                    send_images = False # æ˜¾å¼ç¡®ä¿é™çº§
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆæ³¨æ„ï¼šattemptå·²ç»+1äº†ï¼‰
                if not is_infinite and attempt >= max_retries:
                    self.logger.error("Geminiç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥ã€‚å³å°†ç»ˆæ­¢ç¨‹åºã€‚")
                    raise e
                
                await asyncio.sleep(1) # Wait before retrying
        
        return texts # Fallback in case loop finishes unexpectedly

    async def _translate(self, from_lang: str, to_lang: str, queries: List[str], ctx=None) -> List[str]:
        """ä¸»ç¿»è¯‘æ–¹æ³•"""
        if not self.client:
            from .. import manga_translator
            if hasattr(manga_translator, 'config') and hasattr(manga_translator.config, 'translator'):
                self.parse_args(manga_translator.config.translator)

        if not queries:
            return []

        # é‡ç½®å…¨å±€å°è¯•è®¡æ•°å™¨
        self._reset_global_attempt_count()

        # æ£€æŸ¥æ˜¯å¦ä¸ºé«˜è´¨é‡æ‰¹é‡ç¿»è¯‘æ¨¡å¼
        if ctx and hasattr(ctx, 'high_quality_batch_data'):
            batch_data = ctx.high_quality_batch_data
            if batch_data and len(batch_data) > 0:
                self.logger.info(f"é«˜è´¨é‡ç¿»è¯‘æ¨¡å¼ï¼šæ­£åœ¨æ‰“åŒ… {len(batch_data)} å¼ å›¾ç‰‡å¹¶å‘é€ï¼Œæœ€å¤§å°è¯•æ¬¡æ•°: {self._max_total_attempts}...")
                custom_prompt_json = getattr(ctx, 'custom_prompt_json', None)
                line_break_prompt_json = getattr(ctx, 'line_break_prompt_json', None)

                # ä½¿ç”¨åˆ†å‰²åŒ…è£…å™¨è¿›è¡Œç¿»è¯‘
                translations = await self._translate_with_split(
                    self._translate_batch_high_quality,
                    queries,
                    split_level=0,
                    batch_data=batch_data,
                    source_lang=from_lang,
                    target_lang=to_lang,
                    custom_prompt_json=custom_prompt_json,
                    line_break_prompt_json=line_break_prompt_json,
                    ctx=ctx
                )

                # åº”ç”¨æ–‡æœ¬åå¤„ç†ï¼ˆä¸æ™®é€šç¿»è¯‘å™¨ä¿æŒä¸€è‡´ï¼‰
                translations = [self._clean_translation_output(q, r, to_lang) for q, r in zip(queries, translations)]
                return translations
        
        # æ™®é€šå•æ–‡æœ¬ç¿»è¯‘ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        if not self.client:
            self._setup_client()
        
        if not self.client:
            self.logger.error("Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ GEMINI_API_KEY æ˜¯å¦å·²åœ¨UIæˆ–.envæ–‡ä»¶ä¸­æ­£ç¡®è®¾ç½®ã€‚")
            return queries
        
        try:
            simple_prompt = f"Translate the following {from_lang} text to {to_lang}. Provide only the translation:\n\n" + "\n".join(queries)
            
            # åŠ¨æ€æ„å»ºè¯·æ±‚å‚æ•° - é»˜è®¤æ€»æ˜¯å‘é€å®‰å…¨è®¾ç½®
            request_args = {
                "contents": simple_prompt,
                "safety_settings": self.safety_settings
            }

            def generate_content_with_logging(**kwargs):
                log_kwargs = kwargs.copy()
                if 'contents' in log_kwargs and isinstance(log_kwargs['contents'], list):
                    serializable_contents = []
                    for item in log_kwargs['contents']:
                        if isinstance(item, Image.Image):
                            serializable_contents.append(f"<PIL.Image.Image size={item.size} mode={item.mode}>")
                        else:
                            serializable_contents.append(item)
                    log_kwargs['contents'] = serializable_contents
                self.logger.info(f"--- Gemini Fallback Request Body ---\n{json.dumps(log_kwargs, indent=2, ensure_ascii=False)}\n------------------------------------")
                return self.client.generate_content(**kwargs)

            # RPMé™åˆ¶
            if self._MAX_REQUESTS_PER_MINUTE > 0:
                import time
                now = time.time()
                delay = 60.0 / self._MAX_REQUESTS_PER_MINUTE
                elapsed = now - GeminiHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self._last_request_ts_key]
                if elapsed < delay:
                    sleep_time = delay - elapsed
                    self.logger.info(f'Ratelimit sleep: {sleep_time:.2f}s')
                    await asyncio.sleep(sleep_time)
            
            try:
                # è®¾ç½®5åˆ†é’Ÿè¶…æ—¶
                request_args["request_options"] = {"timeout": 300}
                response = await asyncio.to_thread(
                    generate_content_with_logging,
                    **request_args
                )
            except Exception as e:
                # å¦‚æœæ˜¯å®‰å…¨è®¾ç½®é”™è¯¯ï¼Œå°è¯•ç§»é™¤å®‰å…¨è®¾ç½®åé‡è¯•
                error_message = str(e)
                is_safety_error = any(keyword in error_message.lower() for keyword in [
                    'safety_settings', 'safetysettings', 'harm', 'block', 'safety'
                ]) or "400" in error_message
                
                if is_safety_error and "safety_settings" in request_args:
                    self.logger.warning(f"åå¤‡ç¿»è¯‘æ£€æµ‹åˆ°å®‰å…¨è®¾ç½®é”™è¯¯ï¼Œç§»é™¤å®‰å…¨è®¾ç½®åé‡è¯•: {error_message}")
                    request_args = {k: v for k, v in request_args.items() if k != "safety_settings"}
                    response = await asyncio.to_thread(
                        generate_content_with_logging,
                        **request_args
                    )
                else:
                    raise
            
            # åœ¨APIè°ƒç”¨æˆåŠŸåç«‹å³æ›´æ–°æ—¶é—´æˆ³ï¼Œç¡®ä¿æ‰€æœ‰è¯·æ±‚ï¼ˆåŒ…æ‹¬é‡è¯•ï¼‰éƒ½è¢«è®¡å…¥é€Ÿç‡é™åˆ¶
            if self._MAX_REQUESTS_PER_MINUTE > 0:
                GeminiHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self._last_request_ts_key] = time.time()
            
            # éªŒè¯å“åº”å¯¹è±¡æ˜¯å¦æœ‰æ•ˆ
            validate_gemini_response(response, self.logger)
            
            if response and response.text:
                result = response.text.strip()
                translations = result.split('\n')
                translations = [t.strip() for t in translations if t.strip()]
                
                # Strict validation: must match input count
                if len(translations) != len(queries):
                    error_msg = f"Translation count mismatch: expected {len(queries)}, got {len(translations)}"
                    self.logger.error(error_msg)
                    self.logger.error(f"Queries: {queries}")
                    self.logger.error(f"Translations: {translations}")
                    raise Exception(error_msg)
                
                return translations
                
        except Exception as e:
            self.logger.error(f"Geminiç¿»è¯‘å‡ºé”™: {e}")
        
        return queries
