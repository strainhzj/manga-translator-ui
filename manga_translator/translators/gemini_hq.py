import os
import re
import asyncio
import base64
import json
from io import BytesIO
from typing import List, Dict, Any
from PIL import Image
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from .common import CommonTranslator, VALID_LANGUAGES
from .keys import GEMINI_API_KEY
from ..utils import Context


def encode_image_for_gemini(image, max_size=1024):
    """å°†å›¾ç‰‡å¤„ç†ä¸ºé€‚åˆGemini APIçš„æ ¼å¼"""
    # è½¬æ¢å›¾ç‰‡æ ¼å¼ä¸ºRGBï¼ˆå¤„ç†æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡æ¨¡å¼ï¼‰
    if image.mode == "P":
        # è°ƒè‰²æ¿æ¨¡å¼ï¼šè½¬æ¢ä¸ºRGBAï¼ˆå¦‚æœæœ‰é€æ˜åº¦ï¼‰æˆ–RGB
        image = image.convert("RGBA" if "transparency" in image.info else "RGB")

    if image.mode == "RGBA":
        # RGBAæ¨¡å¼ï¼šåˆ›å»ºç™½è‰²èƒŒæ™¯å¹¶åˆå¹¶é€æ˜é€šé“
        background = Image.new('RGB', image.size, (255, 255, 255))
        background.paste(image, mask=image.split()[-1])
        image = background
    elif image.mode in ("LA", "L", "1", "CMYK"):
        # LAï¼ˆç°åº¦+é€æ˜ï¼‰ã€Lï¼ˆç°åº¦ï¼‰ã€1ï¼ˆäºŒå€¼ï¼‰ã€CMYKï¼šç»Ÿä¸€è½¬æ¢ä¸ºRGB
        if image.mode == "LA":
            # ç°åº¦+é€æ˜ï¼šå…ˆè½¬RGBAå†åˆå¹¶åˆ°ç™½è‰²èƒŒæ™¯
            image = image.convert("RGBA")
            background = Image.new('RGB', image.size, (255, 255, 255))
            background.paste(image, mask=image.split()[-1])
            image = background
        else:
            # å…¶ä»–æ¨¡å¼ï¼šç›´æ¥è½¬RGB
            image = image.convert("RGB")
    elif image.mode != "RGB":
        # å…¶ä»–æœªçŸ¥æ¨¡å¼ï¼šå¼ºåˆ¶è½¬æ¢ä¸ºRGB
        image = image.convert("RGB")

    # è°ƒæ•´å›¾ç‰‡å¤§å°
    w, h = image.size
    if max(w, h) > max_size:
        scale = max_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        image = image.resize((new_w, new_h), Image.LANCZOS)

    return image


def _flatten_prompt_data(data: Any, indent: int = 0) -> str:
    """Recursively flattens a dictionary or list into a formatted string."""
    prompt_parts = []
    prefix = "  " * indent

    if isinstance(data, dict):
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                prompt_parts.append(f"{prefix}- {key}:")
                prompt_parts.append(_flatten_prompt_data(value, indent + 1))
            else:
                prompt_parts.append(f"{prefix}- {key}: {value}")
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, (dict, list)):
                prompt_parts.append(_flatten_prompt_data(item, indent + 1))
            else:
                prompt_parts.append(f"{prefix}- {item}")
    
    return "\n".join(prompt_parts)

class GeminiHighQualityTranslator(CommonTranslator):
    """
    Geminié«˜è´¨é‡ç¿»è¯‘å™¨
    æ”¯æŒå¤šå›¾ç‰‡æ‰¹é‡å¤„ç†ï¼Œæä¾›æ–‡æœ¬æ¡†é¡ºåºã€åŸæ–‡å’ŒåŸå›¾ç»™AIè¿›è¡Œæ›´ç²¾å‡†çš„ç¿»è¯‘
    """
    _LANGUAGE_CODE_MAP = VALID_LANGUAGES
    
    def __init__(self):
        super().__init__()
        self.client = None
        # Initial setup from environment variables
        # é‡æ–°åŠ è½½ .env æ–‡ä»¶ä»¥è·å–æœ€æ–°é…ç½®
        from dotenv import load_dotenv
        load_dotenv(override=True)
        self.api_key = os.getenv('GEMINI_API_KEY', GEMINI_API_KEY)
        self.base_url = os.getenv('GEMINI_API_BASE', 'https://generativelanguage.googleapis.com')
        self.model_name = os.getenv('GEMINI_MODEL', "gemini-1.5-flash")
        self.max_tokens = 25000
        self.temperature = 0.1
        self.safety_settings = [
            {
                "category": HarmCategory.HARM_CATEGORY_HARASSMENT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
        ]
        self._setup_client()
        
    def _setup_client(self):
        """è®¾ç½®Geminiå®¢æˆ·ç«¯"""
        if not self.client and self.api_key:
            client_options = {"api_endpoint": self.base_url} if self.base_url else None

            genai.configure(
                api_key=self.api_key,
                transport='rest',  # æ”¯æŒè‡ªå®šä¹‰base_url
                client_options=client_options
            )
            
            # Apply different configs for different API types
            is_third_party_api = self.base_url and self.base_url != 'https://generativelanguage.googleapis.com'

            if not is_third_party_api:
                # Official Google API - full config
                generation_config = {
                    "temperature": self.temperature,
                    "top_p": 0.95,
                    "top_k": 64,
                    "max_output_tokens": self.max_tokens,
                    "response_mime_type": "text/plain",
                }
                model_args = {
                    "model_name": self.model_name,
                    "generation_config": generation_config,
                    "safety_settings": self.safety_settings
                }
                self.logger.info("ä½¿ç”¨å®˜æ–¹Google APIï¼Œåº”ç”¨å®Œæ•´é…ç½®ã€‚")
            else:
                # Third-party API - minimal config to avoid format issues
                generation_config = {
                    "temperature": self.temperature,
                    "max_output_tokens": self.max_tokens,
                }
                model_args = {
                    "model_name": self.model_name,
                    "generation_config": generation_config,
                }
                self.logger.info("æ£€æµ‹åˆ°ç¬¬ä¸‰æ–¹APIï¼Œä½¿ç”¨ç®€åŒ–é…ç½®é¿å…æ ¼å¼å†²çªã€‚")

            self.client = genai.GenerativeModel(**model_args)
    

    
    def _build_system_prompt(self, source_lang: str, target_lang: str, custom_prompt_json: Dict[str, Any] = None, line_break_prompt_json: Dict[str, Any] = None) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        # Map language codes to full names for clarity in the prompt
        lang_map = {
            "CHS": "Simplified Chinese",
            "CHT": "Traditional Chinese",
            "JPN": "Japanese",
            "ENG": "English",
            "KOR": "Korean",
            "VIN": "Vietnamese",
            "FRA": "French",
            "DEU": "German",
            "ITA": "Italian",
        }
        target_lang_full = lang_map.get(target_lang, target_lang) # Fallback to the code itself

        custom_prompt_str = ""
        if custom_prompt_json:
            custom_prompt_str = _flatten_prompt_data(custom_prompt_json)
            # self.logger.info(f"--- Custom Prompt Content ---\n{custom_prompt_str}\n---------------------------")

        line_break_prompt_str = ""
        if line_break_prompt_json and line_break_prompt_json.get('line_break_prompt'):
            line_break_prompt_str = line_break_prompt_json['line_break_prompt']

        try:
            from ..utils import BASE_PATH
            import os
            import json
            prompt_path = os.path.join(BASE_PATH, 'dict', 'system_prompt_hq.json')
            with open(prompt_path, 'r', encoding='utf-8') as f:
                base_prompt_data = json.load(f)
            base_prompt = base_prompt_data['system_prompt']
        except Exception as e:
            self.logger.warning(f"Failed to load system prompt from file, falling back to hardcoded prompt. Error: {e}")
            base_prompt = f"""You are an expert manga translator. Your task is to accurately translate manga text from the source language into **{{{target_lang}}}**. You will be given the full manga page for context.\n\n**CRITICAL INSTRUCTIONS (FOLLOW STRICTLY):**\n\n1.  **DIRECT TRANSLATION ONLY**: Your output MUST contain ONLY the raw, translated text. Nothing else.\n    -   DO NOT include the original text.\n    -   DO NOT include any explanations, greetings, apologies, or any conversational text.\n    -   DO NOT use Markdown formatting (like ```json or ```).\n    -   The output is fed directly to an automated script. Any extra text will cause it to fail.\n\n2.  **MATCH LINE COUNT**: The number of lines in your output MUST EXACTLY match the number of text regions you are asked to translate. Each line in your output corresponds to one numbered text region in the input.\n\n3.  **TRANSLATE EVERYTHING**: Translate all text provided, including sound effects and single characters. Do not leave any line untranslated.\n\n4.  **ACCURACY AND TONE**:\n    -   Preserve the original tone, emotion, and character's voice.\n    -   Ensure consistent translation of names, places, and special terms.\n    -   For onomatopoeia (sound effects), provide the equivalent sound in {{{target_lang}}} or a brief description (e.g., '(rumble)', '(thud)').\n\n---\n\n**EXAMPLE OF CORRECT AND INCORRECT OUTPUT:**\n\n**[ CORRECT OUTPUT EXAMPLE ]**\nThis is a correct response. Notice it only contains the translated text, with each translation on a new line.\n\n(Imagine the user input was: "1. ã†ã‚‹ã•ã„ï¼", "2. é»™ã‚Œï¼")\n```\nåµæ­»äº†ï¼\né—­å˜´ï¼\n```\n\n**[ âŒ INCORRECT OUTPUT EXAMPLE ]**\nThis is an incorrect response because it includes extra text and explanations.\n\n(Imagine the user input was: "1. ã†ã‚‹ã•ã„ï¼", "2. é»™ã‚Œï¼")\n```\nå¥½çš„ï¼Œè¿™æ˜¯æ‚¨çš„ç¿»è¯‘ï¼š\n1. åµæ­»äº†ï¼\n2. é—­å˜´ï¼\n```\n**REASONING:** The above example is WRONG because it includes "å¥½çš„ï¼Œè¿™æ˜¯æ‚¨çš„ç¿»è¯‘ï¼š" and numbering. Your response must be ONLY the translated text, line by line.\n\n---\n\n**FINAL INSTRUCTION:** Now, perform the translation task. Remember, your response must be clean, containing only the translated text.\n"""

        # Replace placeholder with the full language name
        base_prompt = base_prompt.replace("{{{target_lang}}}", target_lang_full)

        # Combine prompts
        final_prompt = ""
        if line_break_prompt_str:
            final_prompt += f"{line_break_prompt_str}\n\n---\n\n"
        if custom_prompt_str:
            final_prompt += f"{custom_prompt_str}\n\n---\n\n"
        
        final_prompt += base_prompt
        return final_prompt

    def _build_user_prompt(self, batch_data: List[Dict], ctx: Any) -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        # æ£€æŸ¥æ˜¯å¦å¼€å¯AIæ–­å¥
        enable_ai_break = False
        if ctx and hasattr(ctx, 'config') and ctx.config and hasattr(ctx.config, 'render'):
            enable_ai_break = getattr(ctx.config.render, 'disable_auto_wrap', False)

        prompt = "Please translate the following manga text regions. I'm providing multiple images with their text regions in reading order:\n\n"
        
        # æ·»åŠ å›¾ç‰‡ä¿¡æ¯
        for i, data in enumerate(batch_data):
            prompt += f"=== Image {i+1} ===\n"
            prompt += f"Text regions ({len(data['original_texts'])} regions):\n"
            for j, text in enumerate(data['original_texts']):
                prompt += f"  {j+1}. {text}\n"
            prompt += "\n"
        
        prompt += "All texts to translate (in order):\n"
        text_index = 1
        for img_idx, data in enumerate(batch_data):
            for region_idx, text in enumerate(data['original_texts']):
                text_to_translate = text.replace('\n', ' ').replace('\ufffd', '')
                # åªæœ‰å¼€å¯AIæ–­å¥æ—¶æ‰æ·»åŠ åŒºåŸŸä¿¡æ¯
                if enable_ai_break and data['text_regions'] and region_idx < len(data['text_regions']):
                    region = data['text_regions'][region_idx]
                    region_count = len(region.lines) if hasattr(region, 'lines') else 1
                    prompt += f"{text_index}. [Original regions: {region_count}] {text_to_translate}\n"
                else:
                    prompt += f"{text_index}. {text_to_translate}\n"
                text_index += 1
        
        prompt += "\nCRITICAL: Provide translations in the exact same order as the numbered input text regions. Your first line of output must be the translation for text region #1, your second line for #2, and so on. DO NOT CHANGE THE ORDER."
        
        return prompt

    async def _translate_batch_high_quality(self, texts: List[str], batch_data: List[Dict], source_lang: str, target_lang: str, custom_prompt_json: Dict[str, Any] = None, line_break_prompt_json: Dict[str, Any] = None, ctx: Any = None) -> List[str]:
        """é«˜è´¨é‡æ‰¹é‡ç¿»è¯‘æ–¹æ³•"""
        if not texts:
            return []
        
        if not self.client:
            self._setup_client()
        
        if not self.client:
            self.logger.error("Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
            return texts
        
        # å‡†å¤‡å›¾ç‰‡å’Œå†…å®¹
        content_parts = []
        
        # æ‰“å°è¾“å…¥çš„åŸæ–‡
        self.logger.info("--- Original Texts for Translation ---")
        for i, text in enumerate(texts):
            self.logger.info(f"{i+1}: {text}")
        self.logger.info("------------------------------------")

        # æ‰“å°å›¾ç‰‡ä¿¡æ¯
        self.logger.info("--- Image Info ---")
        for i, data in enumerate(batch_data):
            image = data['image']
            self.logger.info(f"Image {i+1}: size={image.size}, mode={image.mode}")
        self.logger.info("--------------------")

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯
        system_prompt = self._build_system_prompt(source_lang, target_lang, custom_prompt_json=custom_prompt_json, line_break_prompt_json=line_break_prompt_json)
        user_prompt = self._build_user_prompt(batch_data, ctx)
        
        content_parts.append(system_prompt + "\n\n" + user_prompt)
        
        # æ·»åŠ å›¾ç‰‡
        for data in batch_data:
            image = data['image']
            processed_image = encode_image_for_gemini(image)
            content_parts.append(processed_image)
        
        # å‘é€è¯·æ±‚
        max_retries = self.attempts
        attempt = 0
        is_infinite = max_retries == -1

        # Dynamically construct arguments for generate_content
        request_args = {
            "contents": content_parts
        }
        is_third_party_api = self.base_url and self.base_url != 'https://generativelanguage.googleapis.com'
        if is_third_party_api:
            self.logger.warning("Omitting safety settings for third-party API request.")
        else:
            request_args["safety_settings"] = self.safety_settings

        def generate_content_with_logging(**kwargs):
            # Create a serializable copy of the arguments for logging
            log_kwargs = kwargs.copy()
            if 'contents' in log_kwargs and isinstance(log_kwargs['contents'], list):
                serializable_contents = []
                for item in log_kwargs['contents']:
                    if isinstance(item, Image.Image):
                        serializable_contents.append(f"<PIL.Image.Image size={item.size} mode={item.mode}>")
                    else:
                        serializable_contents.append(item)
                log_kwargs['contents'] = serializable_contents

            self.logger.info(f"--- Gemini Request Body ---\n{json.dumps(log_kwargs, indent=2, ensure_ascii=False)}\n---------------------------")
            return self.client.generate_content(**kwargs)

        while is_infinite or attempt < max_retries:
            try:
                response = await asyncio.to_thread(
                    generate_content_with_logging,
                    **request_args
                )

                # æ£€æŸ¥finish_reasonï¼Œåªæœ‰æˆåŠŸ(1)æ‰ç»§ç»­ï¼Œå…¶ä»–éƒ½é‡è¯•
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = candidate.finish_reason

                        if finish_reason != 1:  # ä¸æ˜¯STOP(æˆåŠŸ)
                            attempt += 1
                            log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"

                            # æ˜¾ç¤ºå…·ä½“çš„finish_reasonä¿¡æ¯
                            finish_reason_map = {
                                1: "STOP(æˆåŠŸ)",
                                2: "SAFETY(å®‰å…¨ç­–ç•¥æ‹¦æˆª)",
                                3: "MAX_TOKENS(è¾¾åˆ°æœ€å¤§tokené™åˆ¶)",
                                4: "RECITATION(å†…å®¹é‡å¤æ£€æµ‹)",
                                5: "OTHER(å…¶ä»–æœªçŸ¥é”™è¯¯)"
                            }
                            reason_desc = finish_reason_map.get(finish_reason, f"æœªçŸ¥é”™è¯¯ç ({finish_reason})")

                            self.logger.warning(f"Gemini APIå¤±è´¥ ({log_attempt}): finish_reason={finish_reason} - {reason_desc}")

                            if not is_infinite and attempt >= max_retries:
                                self.logger.error(f"Geminiç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»å¤±è´¥: {reason_desc}")
                                break
                            await asyncio.sleep(1)
                            continue

                # å°è¯•è®¿é—® .text å±æ€§ï¼Œå¦‚æœAPIå› å®‰å…¨åŸå› ç­‰è¿”å›ç©ºå†…å®¹ï¼Œè¿™é‡Œä¼šè§¦å‘å¼‚å¸¸
                result_text = response.text.strip()

                # è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°Geminiçš„åŸå§‹è¿”å›å†…å®¹
                self.logger.info(f"--- Gemini Raw Response ---\n{result_text}\n---------------------------")

                # å¢åŠ æ¸…ç†æ­¥éª¤ï¼Œç§»é™¤å¯èƒ½çš„Markdownä»£ç å—
                if result_text.startswith("```") and result_text.endswith("```"):
                    result_text = result_text[3:-3].strip()
                
                # å¦‚æœæˆåŠŸè·å–æ–‡æœ¬ï¼Œåˆ™å¤„ç†å¹¶è¿”å›
                translations = []
                for line in result_text.split('\n'):
                    line = line.strip()
                    if line:
                        # ç§»é™¤ç¼–å·ï¼ˆå¦‚"1. "ï¼‰
                        line = re.sub(r'^\d+\.\s*', '', line)
                        # Replace other possible newline representations, but keep [BR]
                        line = line.replace('\\n', '\n').replace('â†µ', '\n')
                        translations.append(line)
                
                # ç¡®ä¿ç¿»è¯‘æ•°é‡åŒ¹é… - ç”¨ç©ºå­—ç¬¦ä¸²å¡«å……è€Œä¸æ˜¯åŸæ–‡
                while len(translations) < len(texts):
                    translations.append("")
                
                # æ‰“å°åŸæ–‡å’Œè¯‘æ–‡çš„å¯¹åº”å…³ç³»
                self.logger.info("--- Translation Results ---")
                for original, translated in zip(texts, translations):
                    self.logger.info(f'{original} -> {translated}')
                self.logger.info("---------------------------")

                return translations[:len(texts)]

            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯400é”™è¯¯æˆ–å¤šæ¨¡æ€ä¸æ”¯æŒé—®é¢˜
                error_message = str(e)
                is_bad_request = '400' in error_message or 'BadRequest' in error_message
                is_multimodal_unsupported = any(keyword in error_message.lower() for keyword in [
                    'image_url', 'multimodal', 'vision', 'expected `text`', 'unknown variant', 'does not support'
                ])
                
                if is_bad_request and is_multimodal_unsupported:
                    self.logger.error(f"âŒ æ¨¡å‹ {self.model} ä¸æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾ç‰‡+æ–‡æœ¬ï¼‰")
                    self.logger.error(f"ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
                    self.logger.error(f"   1. ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„Geminiæ¨¡å‹ï¼ˆå¦‚ gemini-1.5-flash, gemini-1.5-proï¼‰")
                    self.logger.error(f"   2. æˆ–è€…åˆ‡æ¢åˆ°æ™®é€šç¿»è¯‘æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ _hq é«˜è´¨é‡ç¿»è¯‘å™¨ï¼‰")
                    self.logger.error(f"   3. æ£€æŸ¥ç¬¬ä¸‰æ–¹APIæ˜¯å¦æ”¯æŒå›¾ç‰‡è¾“å…¥")
                    raise Exception(f"æ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€è¾“å…¥: {self.model}") from e
                    
                attempt += 1
                log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                self.logger.warning(f"Geminié«˜è´¨é‡ç¿»è¯‘å‡ºé”™ ({log_attempt}): {e}")

                if "finish_reason: 2" in error_message or "finish_reason is 2" in error_message:
                    self.logger.warning("æ£€æµ‹åˆ°Geminiå®‰å…¨è®¾ç½®æ‹¦æˆªã€‚æ­£åœ¨é‡è¯•...")
                
                if not is_infinite and attempt >= max_retries:
                    self.logger.error("Geminiç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥ã€‚å³å°†ç»ˆæ­¢ç¨‹åºã€‚")
                    raise e
                
                await asyncio.sleep(1) # Wait before retrying
        
        return texts # Fallback in case loop finishes unexpectedly

    async def _translate(self, from_lang: str, to_lang: str, queries: List[str], ctx=None) -> List[str]:
        """ä¸»ç¿»è¯‘æ–¹æ³•"""
        if not self.client:
            from .. import manga_translator
            if hasattr(manga_translator, 'config') and hasattr(manga_translator.config, 'translator'):
                self.parse_args(manga_translator.config.translator)
        
        if not queries:
            return []
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé«˜è´¨é‡æ‰¹é‡ç¿»è¯‘æ¨¡å¼
        if ctx and hasattr(ctx, 'high_quality_batch_data'):
            batch_data = ctx.high_quality_batch_data
            if batch_data and len(batch_data) > 0:
                self.logger.info(f"é«˜è´¨é‡ç¿»è¯‘æ¨¡å¼ï¼šæ­£åœ¨æ‰“åŒ… {len(batch_data)} å¼ å›¾ç‰‡å¹¶å‘é€...")
                custom_prompt_json = getattr(ctx, 'custom_prompt_json', None)
                line_break_prompt_json = getattr(ctx, 'line_break_prompt_json', None)
                translations = await self._translate_batch_high_quality(queries, batch_data, from_lang, to_lang, custom_prompt_json=custom_prompt_json, line_break_prompt_json=line_break_prompt_json, ctx=ctx)
                # åº”ç”¨æ–‡æœ¬åå¤„ç†ï¼ˆä¸æ™®é€šç¿»è¯‘å™¨ä¿æŒä¸€è‡´ï¼‰
                translations = [self._clean_translation_output(q, r, to_lang) for q, r in zip(queries, translations)]
                return translations
        
        # æ™®é€šå•æ–‡æœ¬ç¿»è¯‘ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        if not self.client:
            self._setup_client()
        
        if not self.client:
            self.logger.error("Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ GEMINI_API_KEY æ˜¯å¦å·²åœ¨UIæˆ–.envæ–‡ä»¶ä¸­æ­£ç¡®è®¾ç½®ã€‚")
            return queries
        
        try:
            simple_prompt = f"Translate the following {from_lang} text to {to_lang}. Provide only the translation:\n\n" + "\n".join(queries)
            
            # Dynamically construct arguments to handle safety settings for the fallback path
            request_args = {
                "contents": simple_prompt
            }
            is_third_party_api = self.base_url and self.base_url != 'https://generativelanguage.googleapis.com'
            if is_third_party_api:
                # For third-party APIs, omit the safety_settings parameter entirely
                pass
            else:
                request_args["safety_settings"] = self.safety_settings

            def generate_content_with_logging(**kwargs):
                log_kwargs = kwargs.copy()
                if 'contents' in log_kwargs and isinstance(log_kwargs['contents'], list):
                    serializable_contents = []
                    for item in log_kwargs['contents']:
                        if isinstance(item, Image.Image):
                            serializable_contents.append(f"<PIL.Image.Image size={item.size} mode={item.mode}>")
                        else:
                            serializable_contents.append(item)
                    log_kwargs['contents'] = serializable_contents
                self.logger.info(f"--- Gemini Fallback Request Body ---\n{json.dumps(log_kwargs, indent=2, ensure_ascii=False)}\n------------------------------------")
                return self.client.generate_content(**kwargs)

            response = await asyncio.to_thread(
                generate_content_with_logging,
                **request_args
            )
            
            if response and response.text:
                result = response.text.strip()
                translations = result.split('\n')
                translations = [t.strip() for t in translations if t.strip()]
                
                # ç¡®ä¿æ•°é‡åŒ¹é…
                while len(translations) < len(queries):
                    translations.append(queries[len(translations)] if len(translations) < len(queries) else "")
                
                return translations[:len(queries)]
                
        except Exception as e:
            self.logger.error(f"Geminiç¿»è¯‘å‡ºé”™: {e}")
        
        return queries
