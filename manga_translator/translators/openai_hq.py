import os
import re
import asyncio
import base64
import json
import logging
from io import BytesIO
from typing import List, Dict, Any
from PIL import Image
import openai
from openai import AsyncOpenAI

from .common import CommonTranslator, VALID_LANGUAGES
from .keys import OPENAI_API_KEY, OPENAI_MODEL
from ..utils import Context

# ç¦ç”¨openaiåº“çš„DEBUGæ—¥å¿—,é¿å…æ‰“å°base64å›¾ç‰‡æ•°æ®
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)


def encode_image_for_openai(image, max_size=1024):
    """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64æ ¼å¼ï¼Œé€‚åˆOpenAI API"""
    # è½¬æ¢å›¾ç‰‡æ ¼å¼ä¸ºRGBï¼ˆå¤„ç†æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡æ¨¡å¼ï¼‰
    if image.mode == "P":
        # è°ƒè‰²æ¿æ¨¡å¼ï¼šè½¬æ¢ä¸ºRGBAï¼ˆå¦‚æœæœ‰é€æ˜åº¦ï¼‰æˆ–RGB
        image = image.convert("RGBA" if "transparency" in image.info else "RGB")

    if image.mode == "RGBA":
        # RGBAæ¨¡å¼ï¼šåˆ›å»ºç™½è‰²èƒŒæ™¯å¹¶åˆå¹¶é€æ˜é€šé“
        background = Image.new('RGB', image.size, (255, 255, 255))
        background.paste(image, mask=image.split()[-1])
        image = background
    elif image.mode in ("LA", "L", "1", "CMYK"):
        # LAï¼ˆç°åº¦+é€æ˜ï¼‰ã€Lï¼ˆç°åº¦ï¼‰ã€1ï¼ˆäºŒå€¼ï¼‰ã€CMYKï¼šç»Ÿä¸€è½¬æ¢ä¸ºRGB
        if image.mode == "LA":
            # ç°åº¦+é€æ˜ï¼šå…ˆè½¬RGBAå†åˆå¹¶åˆ°ç™½è‰²èƒŒæ™¯
            image = image.convert("RGBA")
            background = Image.new('RGB', image.size, (255, 255, 255))
            background.paste(image, mask=image.split()[-1])
            image = background
        else:
            # å…¶ä»–æ¨¡å¼ï¼šç›´æ¥è½¬RGB
            image = image.convert("RGB")
    elif image.mode != "RGB":
        # å…¶ä»–æœªçŸ¥æ¨¡å¼ï¼šå¼ºåˆ¶è½¬æ¢ä¸ºRGB
        image = image.convert("RGB")

    # è°ƒæ•´å›¾ç‰‡å¤§å°
    w, h = image.size
    if max(w, h) > max_size:
        scale = max_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        image = image.resize((new_w, new_h), Image.LANCZOS)

    # ç¼–ç ä¸ºbase64ï¼ˆä½¿ç”¨PNGæ ¼å¼ç¡®ä¿è´¨é‡å’Œå…¼å®¹æ€§ï¼‰
    buf = BytesIO()
    image.save(buf, format="PNG")
    return base64.b64encode(buf.getvalue()).decode('utf-8')


def _flatten_prompt_data(data: Any, indent: int = 0) -> str:
    """Recursively flattens a dictionary or list into a formatted string."""
    prompt_parts = []
    prefix = "  " * indent

    if isinstance(data, dict):
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                prompt_parts.append(f"{prefix}- {key}:")
                prompt_parts.append(_flatten_prompt_data(value, indent + 1))
            else:
                prompt_parts.append(f"{prefix}- {key}: {value}")
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, (dict, list)):
                prompt_parts.append(_flatten_prompt_data(item, indent + 1))
            else:
                prompt_parts.append(f"{prefix}- {item}")
    
    return "\n".join(prompt_parts)

class OpenAIHighQualityTranslator(CommonTranslator):
    """
    OpenAIé«˜è´¨é‡ç¿»è¯‘å™¨
    æ”¯æŒå¤šå›¾ç‰‡æ‰¹é‡å¤„ç†ï¼Œæä¾›æ–‡æœ¬æ¡†é¡ºåºã€åŸæ–‡å’ŒåŸå›¾ç»™AIè¿›è¡Œæ›´ç²¾å‡†çš„ç¿»è¯‘
    """
    _LANGUAGE_CODE_MAP = VALID_LANGUAGES
    
    # ç±»å˜é‡: è·¨å®ä¾‹å…±äº«çš„RPMé™åˆ¶æ—¶é—´æˆ³
    _GLOBAL_LAST_REQUEST_TS = {}  # {model_name: timestamp}
    
    def __init__(self):
        super().__init__()
        self.client = None
        self.prev_context = ""  # ç”¨äºå­˜å‚¨å¤šé¡µä¸Šä¸‹æ–‡
        
        # åªåœ¨éWebç¯å¢ƒä¸‹é‡æ–°åŠ è½½.envæ–‡ä»¶
        # Webç¯å¢ƒä¸‹ä¸é‡æ–°åŠ è½½ï¼Œé¿å…è¦†ç›–ç”¨æˆ·ä¸´æ—¶è®¾ç½®çš„ç¯å¢ƒå˜é‡
        is_web_server = os.getenv('MANGA_TRANSLATOR_WEB_SERVER', 'false').lower() == 'true'
        if not is_web_server:
            from dotenv import load_dotenv
            load_dotenv(override=True)
        
        self.api_key = os.getenv('OPENAI_API_KEY', OPENAI_API_KEY)
        self.base_url = os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1')
        self.model = os.getenv('OPENAI_MODEL', "gpt-4o")
        self.max_tokens = 25000
        self.temperature = 0.1
        self._MAX_REQUESTS_PER_MINUTE = 0  # é»˜è®¤æ— é™åˆ¶
        # ä½¿ç”¨å…¨å±€æ—¶é—´æˆ³,è·¨å®ä¾‹å…±äº«
        if self.model not in OpenAIHighQualityTranslator._GLOBAL_LAST_REQUEST_TS:
            OpenAIHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self.model] = 0
        self._last_request_ts_key = self.model
        self._setup_client()
    
    def set_prev_context(self, context: str):
        """è®¾ç½®å¤šé¡µä¸Šä¸‹æ–‡ï¼ˆç”¨äºcontext_size > 0æ—¶ï¼‰"""
        self.prev_context = context if context else ""
    
    def parse_args(self, args):
        """è§£æé…ç½®å‚æ•°"""
        # ä»é…ç½®ä¸­è¯»å–RPMé™åˆ¶
        max_rpm = getattr(args, 'max_requests_per_minute', 0)
        if max_rpm > 0:
            self._MAX_REQUESTS_PER_MINUTE = max_rpm
            self.logger.info(f"Setting OpenAI HQ max requests per minute to: {max_rpm}")
    
    def _setup_client(self):
        """è®¾ç½®OpenAIå®¢æˆ·ç«¯"""
        if not self.client:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
    
    async def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.client:
            try:
                await self.client.close()
            except Exception:
                pass  # å¿½ç•¥æ¸…ç†æ—¶çš„é”™è¯¯
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«æ¸…ç†"""
        if self.client:
            try:
                import asyncio
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # å¦‚æœäº‹ä»¶å¾ªç¯è¿˜åœ¨è¿è¡Œï¼Œåˆ›å»ºæ¸…ç†ä»»åŠ¡
                    asyncio.create_task(self._cleanup())
                elif not loop.is_closed():
                    # å¦‚æœäº‹ä»¶å¾ªç¯æœªå…³é—­ï¼ŒåŒæ­¥æ‰§è¡Œæ¸…ç†
                    loop.run_until_complete(self._cleanup())
            except Exception:
                pass  # å¿½ç•¥æ‰€æœ‰æ¸…ç†é”™è¯¯

    
    def _build_system_prompt(self, source_lang: str, target_lang: str, custom_prompt_json: Dict[str, Any] = None, line_break_prompt_json: Dict[str, Any] = None) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        # Map language codes to full names for clarity in the prompt
        lang_map = {
            "CHS": "Simplified Chinese",
            "CHT": "Traditional Chinese",
            "JPN": "Japanese",
            "ENG": "English",
            "KOR": "Korean",
            "VIN": "Vietnamese",
            "FRA": "French",
            "DEU": "German",
            "ITA": "Italian",
        }
        target_lang_full = lang_map.get(target_lang, target_lang) # Fallback to the code itself

        custom_prompt_str = ""
        if custom_prompt_json:
            custom_prompt_str = _flatten_prompt_data(custom_prompt_json)
            # self.logger.info(f"--- Custom Prompt Content ---\n{custom_prompt_str}\n---------------------------")

        line_break_prompt_str = ""
        if line_break_prompt_json and line_break_prompt_json.get('line_break_prompt'):
            line_break_prompt_str = line_break_prompt_json['line_break_prompt']

        try:
            from ..utils import BASE_PATH
            import os
            import json
            prompt_path = os.path.join(BASE_PATH, 'dict', 'system_prompt_hq.json')
            with open(prompt_path, 'r', encoding='utf-8') as f:
                base_prompt_data = json.load(f)
            base_prompt = base_prompt_data['system_prompt']
        except Exception as e:
            self.logger.warning(f"Failed to load system prompt from file, falling back to hardcoded prompt. Error: {e}")
            base_prompt = f"""You are an expert manga translator. Your task is to accurately translate manga text from the source language into **{{{target_lang}}}**. You will be given the full manga page for context.

**CRITICAL INSTRUCTIONS (FOLLOW STRICTLY):**

1.  **DIRECT TRANSLATION ONLY**: Your output MUST contain ONLY the raw, translated text. Nothing else.
    -   DO NOT include the original text.
    -   DO NOT include any explanations, greetings, apologies, or any conversational text.
    -   DO NOT use Markdown formatting (like ```json or ```).
    -   The output is fed directly to an automated script. Any extra text will cause it to fail.

2.  **MATCH LINE COUNT**: The number of lines in your output MUST EXACTLY match the number of text regions you are asked to translate. Each line in your output corresponds to one numbered text region in the input.

3.  **TRANSLATE EVERYTHING**: Translate all text provided, including sound effects and single characters. Do not leave any line untranslated.

4.  **ACCURACY AND TONE**:
    -   Preserve the original tone, emotion, and character's voice.
    -   Ensure consistent translation of names, places, and special terms.
    -   For onomatopoeia (sound effects), provide the equivalent sound in {{{target_lang}}} or a brief description (e.g., '(rumble)', '(thud)').

---

**EXAMPLE OF CORRECT AND INCORRECT OUTPUT:**

**[ CORRECT OUTPUT EXAMPLE ]**
This is a correct response. Notice it only contains the translated text, with each translation on a new line.

(Imagine the user input was: "1. ã†ã‚‹ã•ã„ï¼", "2. é»™ã‚Œï¼")
```
åµæ­»äº†ï¼
é—­å˜´ï¼
```

**[ âŒ INCORRECT OUTPUT EXAMPLE ]**
This is an incorrect response because it includes extra text and explanations.

(Imagine the user input was: "1. ã†ã‚‹ã•ã„ï¼", "2. é»™ã‚Œï¼")
```
å¥½çš„ï¼Œè¿™æ˜¯æ‚¨çš„ç¿»è¯‘ï¼š
1. åµæ­»äº†ï¼
2. é—­å˜´ï¼
```
**REASONING:** The above example is WRONG because it includes "å¥½çš„ï¼Œè¿™æ˜¯æ‚¨çš„ç¿»è¯‘ï¼š" and numbering. Your response must be ONLY the translated text, line by line.

---

**FINAL INSTRUCTION:** Now, perform the translation task. Remember, your response must be clean, containing only the translated text.
"""

        # Replace placeholder with the full language name
        base_prompt = base_prompt.replace("{{{target_lang}}}", target_lang_full)

        # Combine prompts
        final_prompt = ""
        if line_break_prompt_str:
            final_prompt += f"{line_break_prompt_str}\n\n---\n\n"
        if custom_prompt_str:
            final_prompt += f"{custom_prompt_str}\n\n---\n\n"
        
        final_prompt += base_prompt
        # self.logger.info(f"--- OpenAI HQ Final System Prompt ---\n{final_prompt}")
        return final_prompt

    def _build_user_prompt(self, batch_data: List[Dict], ctx: Any, retry_attempt: int = 0, retry_reason: str = "") -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆé«˜è´¨é‡ç‰ˆï¼‰- ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•ï¼ŒåªåŒ…å«ä¸Šä¸‹æ–‡å’Œå¾…ç¿»è¯‘æ–‡æœ¬"""
        return self._build_user_prompt_for_hq(batch_data, ctx, self.prev_context, retry_attempt=retry_attempt, retry_reason=retry_reason)
    
    def _get_system_prompt(self, source_lang: str, target_lang: str, custom_prompt_json: Dict[str, Any] = None, line_break_prompt_json: Dict[str, Any] = None) -> str:
        """è·å–å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«æ–­å¥æç¤ºè¯ã€è‡ªå®šä¹‰æç¤ºè¯å’ŒåŸºç¡€ç³»ç»Ÿæç¤ºè¯ï¼‰"""
        return self._build_system_prompt(source_lang, target_lang, custom_prompt_json=custom_prompt_json, line_break_prompt_json=line_break_prompt_json)

    async def _translate_batch_high_quality(self, texts: List[str], batch_data: List[Dict], source_lang: str, target_lang: str, custom_prompt_json: Dict[str, Any] = None, line_break_prompt_json: Dict[str, Any] = None, ctx: Any = None, split_level: int = 0) -> List[str]:
        """é«˜è´¨é‡æ‰¹é‡ç¿»è¯‘æ–¹æ³•"""
        if not texts:
            return []
        
        if not self.client:
            self._setup_client()
        
        # å‡†å¤‡å›¾ç‰‡
        self.logger.info(f"é«˜è´¨é‡ç¿»è¯‘æ¨¡å¼ï¼šæ­£åœ¨æ‰“åŒ… {len(batch_data)} å¼ å›¾ç‰‡å¹¶å‘é€...")

        image_contents = []
        for data in batch_data:
            image = data['image']
            base64_img = encode_image_for_openai(image)
            image_contents.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{base64_img}"}
            })
        
        # æ„å»ºæ¶ˆæ¯ï¼šç³»ç»Ÿæç¤ºè¯æ”¾åœ¨ system roleï¼Œç”¨æˆ·å†…å®¹ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰æ”¾åœ¨ user role
        system_prompt = self._get_system_prompt(source_lang, target_lang, custom_prompt_json=custom_prompt_json, line_break_prompt_json=line_break_prompt_json)
        
        # åˆå§‹åŒ–é‡è¯•ä¿¡æ¯
        retry_attempt = 0
        retry_reason = ""
        
        # å‘é€è¯·æ±‚
        max_retries = self.attempts
        attempt = 0
        is_infinite = max_retries == -1
        last_exception = None
        local_attempt = 0  # æœ¬æ¬¡æ‰¹æ¬¡çš„å°è¯•æ¬¡æ•°

        while is_infinite or attempt < max_retries:
            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            self._check_cancelled()
            
            # æ£€æŸ¥å…¨å±€å°è¯•æ¬¡æ•°
            if not self._increment_global_attempt():
                self.logger.error("Reached global attempt limit. Stopping translation.")
                raise Exception(f"Global attempt limit reached: {self._global_attempt_count}/{self._max_total_attempts}")

            local_attempt += 1
            attempt += 1

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘åˆ†å‰²ï¼ˆé‡è¯•2æ¬¡åï¼‰
            if local_attempt > self._SPLIT_THRESHOLD and len(texts) > 1 and split_level < self._MAX_SPLIT_ATTEMPTS:
                self.logger.warning(f"Triggering split after {local_attempt} local attempts")
                raise self.SplitException(local_attempt, texts)
            
            # æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆåŒ…å«é‡è¯•ä¿¡æ¯ä»¥é¿å…ç¼“å­˜ï¼‰
            user_prompt = self._build_user_prompt(batch_data, ctx, retry_attempt=retry_attempt, retry_reason=retry_reason)
            user_content = [{"type": "text", "text": user_prompt}]
            user_content.extend(image_contents)
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ]

            try:
                # RPMé™åˆ¶
                if self._MAX_REQUESTS_PER_MINUTE > 0:
                    import time
                    now = time.time()
                    delay = 60.0 / self._MAX_REQUESTS_PER_MINUTE
                    elapsed = now - OpenAIHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self._last_request_ts_key]
                    if elapsed < delay:
                        sleep_time = delay - elapsed
                        self.logger.info(f'Ratelimit sleep: {sleep_time:.2f}s')
                        await asyncio.sleep(sleep_time)
                
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )
                
                # åœ¨APIè°ƒç”¨æˆåŠŸåç«‹å³æ›´æ–°æ—¶é—´æˆ³ï¼Œç¡®ä¿æ‰€æœ‰è¯·æ±‚ï¼ˆåŒ…æ‹¬é‡è¯•ï¼‰éƒ½è¢«è®¡å…¥é€Ÿç‡é™åˆ¶
                if self._MAX_REQUESTS_PER_MINUTE > 0:
                    OpenAIHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self._last_request_ts_key] = time.time()

                # æ£€æŸ¥æˆåŠŸæ¡ä»¶
                if response.choices and response.choices[0].message.content and response.choices[0].finish_reason != 'content_filter':
                    result_text = response.choices[0].message.content.strip()
                    
                    # âœ… æ£€æµ‹HTMLé”™è¯¯å“åº”ï¼ˆ404ç­‰ï¼‰- æŠ›å‡ºç‰¹å®šå¼‚å¸¸ä¾›ç»Ÿä¸€é”™è¯¯å¤„ç†
                    if result_text.startswith('<!DOCTYPE') or result_text.startswith('<html') or '<h1>404</h1>' in result_text:
                        raise Exception(f"API_404_ERROR: APIè¿”å›HTMLé”™è¯¯é¡µé¢ - APIåœ°å€({self.base_url})æˆ–æ¨¡å‹({self.model})é…ç½®é”™è¯¯")
                    
                    # å¢åŠ æ¸…ç†æ­¥éª¤ï¼Œç§»é™¤å¯èƒ½çš„Markdownä»£ç å—
                    if result_text.startswith("```") and result_text.endswith("```"):
                        result_text = result_text[3:-3].strip()
                    
                    # è§£æç¿»è¯‘ç»“æœ
                    translations = []
                    for line in result_text.split('\n'):
                        line = line.strip()
                        if line:
                            line = re.sub(r'^\d+\.\s*', '', line)
                            # Replace other possible newline representations, but keep [BR]
                            line = line.replace('\\n', '\n').replace('â†µ', '\n')
                            translations.append(line)
                    
                    # Strict validation: must match input count
                    if len(translations) != len(texts):
                        retry_attempt += 1
                        retry_reason = f"Translation count mismatch: expected {len(texts)}, got {len(translations)}"
                        log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                        self.logger.warning(f"[{log_attempt}] {retry_reason}. Retrying...")
                        self.logger.warning(f"Expected texts: {texts}")
                        self.logger.warning(f"Got translations: {translations}")

                        if not is_infinite and attempt >= max_retries:
                            raise Exception(f"Translation count mismatch after {max_retries} attempts: expected {len(texts)}, got {len(translations)}")

                        await asyncio.sleep(2)
                        continue

                    # è´¨é‡éªŒè¯ï¼šæ£€æŸ¥ç©ºç¿»è¯‘ã€åˆå¹¶ç¿»è¯‘ã€å¯ç–‘ç¬¦å·ç­‰
                    is_valid, error_msg = self._validate_translation_quality(texts, translations)
                    if not is_valid:
                        retry_attempt += 1
                        retry_reason = f"Quality check failed: {error_msg}"
                        log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                        self.logger.warning(f"[{log_attempt}] {retry_reason}. Retrying...")

                        if not is_infinite and attempt >= max_retries:
                            raise Exception(f"Quality check failed after {max_retries} attempts: {error_msg}")

                        await asyncio.sleep(2)
                        continue

                    self.logger.info("--- Translation Results ---")
                    for original, translated in zip(texts, translations):
                        self.logger.info(f'{original} -> {translated}')
                    self.logger.info("---------------------------")

                    # BRæ£€æŸ¥ï¼šæ£€æŸ¥ç¿»è¯‘ç»“æœæ˜¯å¦åŒ…å«å¿…è¦çš„[BR]æ ‡è®°
                    # BR check: Check if translations contain necessary [BR] markers
                    if not self._validate_br_markers(translations, batch_data=batch_data, ctx=ctx):
                        retry_attempt += 1
                        retry_reason = "BR markers missing in translations"
                        log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                        self.logger.warning(f"[{log_attempt}] {retry_reason}, retrying...")
                        
                        # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæŠ›å‡ºå‹å¥½çš„å¼‚å¸¸
                        if not is_infinite and attempt >= max_retries:
                            from .common import BRMarkersValidationException
                            self.logger.error("OpenAIé«˜è´¨é‡ç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥ï¼šAIæ–­å¥æ£€æŸ¥å¤±è´¥ã€‚")
                            raise BRMarkersValidationException(
                                missing_count=0,  # å…·ä½“æ•°å­—åœ¨_validate_br_markersä¸­å·²è®°å½•
                                total_count=len(texts),
                                tolerance=max(1, len(texts) // 10)
                            )
                        
                        await asyncio.sleep(2)
                        continue

                    return translations[:len(texts)]
                
                # å¦‚æœä¸æˆåŠŸï¼Œåˆ™è®°å½•åŸå› å¹¶å‡†å¤‡é‡è¯•
                attempt += 1
                log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                finish_reason = response.choices[0].finish_reason if response.choices else "N/A"

                if finish_reason == 'content_filter':
                    self.logger.warning(f"OpenAIå†…å®¹è¢«å®‰å…¨ç­–ç•¥æ‹¦æˆª ({log_attempt})ã€‚æ­£åœ¨é‡è¯•...")
                    last_exception = Exception("OpenAI content filter triggered")
                else:
                    self.logger.warning(f"OpenAIè¿”å›ç©ºå†…å®¹æˆ–æ„å¤–çš„ç»“æŸåŸå›  '{finish_reason}' ({log_attempt})ã€‚æ­£åœ¨é‡è¯•...")
                    last_exception = Exception(f"OpenAI returned empty content or unexpected finish_reason: {finish_reason}")

                if not is_infinite and attempt >= max_retries:
                    self.logger.error("OpenAIç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥ã€‚å³å°†ç»ˆæ­¢ç¨‹åºã€‚")
                    raise last_exception
                
                await asyncio.sleep(1)

            except openai.BadRequestError as e:
                # ä¸“é—¨å¤„ç†400é”™è¯¯ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å¤šæ¨¡æ€ä¸æ”¯æŒé—®é¢˜
                error_message = str(e)
                is_multimodal_unsupported = any(keyword in error_message.lower() for keyword in [
                    'image_url', 'multimodal', 'vision', 'expected `text`', 'unknown variant'
                ])
                
                if is_multimodal_unsupported:
                    self.logger.error(f"âŒ æ¨¡å‹ {self.model} ä¸æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾ç‰‡+æ–‡æœ¬ï¼‰")
                    self.logger.error(f"ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
                    self.logger.error(f"   1. ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼ˆå¦‚ gpt-4o, gpt-4-vision-previewï¼‰")
                    self.logger.error(f"   2. æˆ–è€…åˆ‡æ¢åˆ°æ™®é€šç¿»è¯‘æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ _hq é«˜è´¨é‡ç¿»è¯‘å™¨ï¼‰")
                    self.logger.error(f"   3. DeepSeekæ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€ï¼Œè¯·å‹¿ä½¿ç”¨ openai_hq ç¿»è¯‘å™¨")
                    raise Exception(f"æ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€è¾“å…¥: {self.model}") from e
                else:
                    # å…¶ä»–400é”™è¯¯ï¼Œæ­£å¸¸é‡è¯•
                    attempt += 1
                    log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                    last_exception = e
                    self.logger.warning(f"OpenAIé«˜è´¨é‡ç¿»è¯‘å‡ºé”™ ({log_attempt}): {e}")
                    
                    if not is_infinite and attempt >= max_retries:
                        self.logger.error("OpenAIç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥ã€‚å³å°†ç»ˆæ­¢ç¨‹åºã€‚")
                        raise last_exception
                    
                    await asyncio.sleep(1)
                    
            except Exception as e:
                attempt += 1
                log_attempt = f"{attempt}/{max_retries}" if not is_infinite else f"Attempt {attempt}"
                last_exception = e
                self.logger.warning(f"OpenAIé«˜è´¨é‡ç¿»è¯‘å‡ºé”™ ({log_attempt}): {e}")
                
                if not is_infinite and attempt >= max_retries:
                    self.logger.error("OpenAIç¿»è¯‘åœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥ã€‚å³å°†ç»ˆæ­¢ç¨‹åºã€‚")
                    raise last_exception
                
                await asyncio.sleep(1)

        # åªæœ‰åœ¨æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥åæ‰ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
        raise last_exception if last_exception else Exception("OpenAI translation failed after all retries")

    async def _translate(self, from_lang: str, to_lang: str, queries: List[str], ctx=None) -> List[str]:
        """ä¸»ç¿»è¯‘æ–¹æ³•"""
        if not queries:
            return []

        # é‡ç½®å…¨å±€å°è¯•è®¡æ•°å™¨
        self._reset_global_attempt_count()

        # æ£€æŸ¥æ˜¯å¦ä¸ºé«˜è´¨é‡æ‰¹é‡ç¿»è¯‘æ¨¡å¼
        if ctx and hasattr(ctx, 'high_quality_batch_data'):
            batch_data = ctx.high_quality_batch_data
            if batch_data and len(batch_data) > 0:
                self.logger.info(f"ä½¿ç”¨OpenAIé«˜è´¨é‡ç¿»è¯‘æ¨¡å¼å¤„ç†{len(batch_data)}å¼ å›¾ç‰‡ï¼Œæœ€å¤§å°è¯•æ¬¡æ•°: {self._max_total_attempts}")
                custom_prompt_json = getattr(ctx, 'custom_prompt_json', None)
                line_break_prompt_json = getattr(ctx, 'line_break_prompt_json', None)

                # ä½¿ç”¨åˆ†å‰²åŒ…è£…å™¨è¿›è¡Œç¿»è¯‘
                translations = await self._translate_with_split(
                    self._translate_batch_high_quality,
                    queries,
                    split_level=0,
                    batch_data=batch_data,
                    source_lang=from_lang,
                    target_lang=to_lang,
                    custom_prompt_json=custom_prompt_json,
                    line_break_prompt_json=line_break_prompt_json,
                    ctx=ctx
                )

                # åº”ç”¨æ–‡æœ¬åå¤„ç†ï¼ˆä¸æ™®é€šç¿»è¯‘å™¨ä¿æŒä¸€è‡´ï¼‰
                translations = [self._clean_translation_output(q, r, to_lang) for q, r in zip(queries, translations)]
                return translations
        
        # æ™®é€šå•æ–‡æœ¬ç¿»è¯‘ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        if not self.client:
            self._setup_client()
        
        try:
            simple_prompt = f"Translate the following {from_lang} text to {to_lang}. Provide only the translation:\n\n" + "\n".join(queries)
            
            # RPMé™åˆ¶
            if self._MAX_REQUESTS_PER_MINUTE > 0:
                import time
                now = time.time()
                delay = 60.0 / self._MAX_REQUESTS_PER_MINUTE
                elapsed = now - OpenAIHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self._last_request_ts_key]
                if elapsed < delay:
                    sleep_time = delay - elapsed
                    self.logger.info(f'Ratelimit sleep: {sleep_time:.2f}s')
                    await asyncio.sleep(sleep_time)
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": simple_prompt}],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            # åœ¨APIè°ƒç”¨æˆåŠŸåç«‹å³æ›´æ–°æ—¶é—´æˆ³ï¼Œç¡®ä¿æ‰€æœ‰è¯·æ±‚ï¼ˆåŒ…æ‹¬é‡è¯•ï¼‰éƒ½è¢«è®¡å…¥é€Ÿç‡é™åˆ¶
            if self._MAX_REQUESTS_PER_MINUTE > 0:
                OpenAIHighQualityTranslator._GLOBAL_LAST_REQUEST_TS[self._last_request_ts_key] = time.time()
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                translations = result.split('\n')
                translations = [t.strip() for t in translations if t.strip()]
                
                # Strict validation: must match input count
                if len(translations) != len(queries):
                    error_msg = f"Translation count mismatch: expected {len(queries)}, got {len(translations)}"
                    self.logger.error(error_msg)
                    self.logger.error(f"Queries: {queries}")
                    self.logger.error(f"Translations: {translations}")
                    raise Exception(error_msg)
                
                return translations
                
        except Exception as e:
            self.logger.error(f"OpenAIç¿»è¯‘å‡ºé”™: {e}")
        
        return queries